# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1U2V84xO-pwE9Im-C0AKRDlWhnAewpIQB
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
# %matplotlib inline

#Load dataset
df=pd.read_csv('/content/housing.csv')
df.isnull().sum()

from sklearn import preprocessing
sx = preprocessing.MinMaxScaler()
sy = preprocessing.MinMaxScaler()

scaled_X = sx.fit_transform(df.drop(['median_house_value'],axis='columns'))
scaled_y = sy.fit_transform(df['median_house_value'].values.reshape(df.shape[0],1))
scaled_X

scaled_y

#Reshaping 2D array to 1D
scaled_y=scaled_y.reshape(scaled_y.shape[0])
scaled_y

"""MINI BATCH GRADIENT DESCENT"""

scaled_X.shape[0]

def batch_gradient_descent(X, y_true, epochs, learning_rate = 0.01):
 number_of_features = X.shape[1]
 # numpy array with 1 row and columns equal to number of features.

 # our case number_of_features = 2 (area, bedroom)
 w = np.ones(shape=(number_of_features),dtype = int)
 b = 0
 total_samples = X.shape[0] # number of rows in X

 cost_list = []
 epoch_list = []

 for i in range(epochs):
  y_predicted = np.dot(w, X.T) + b
  w_grad = -(2/total_samples)*(X.T.dot(y_true-y_predicted))
  b_grad = -(2/total_samples)*np.sum(y_true-y_predicted)

  w = w - learning_rate * w_grad
  b = b - learning_rate * b_grad

  cost = np.mean(np.square(y_true-y_predicted)) # MSE (Mean Squared Error)

  if i%10==0:
    cost_list.append(cost)
    epoch_list.append(i)

 return w, b, cost, cost_list, epoch_list

w, b, cost, cost_list, epoch_list = batch_gradient_descent(scaled_X, scaled_y, 50000, 0.01)

#Now plot epoch vs cost graph to see how cost reduces as number of epoch increases
plt.xlabel("epoch")
plt.ylabel("cost")
plt.plot(epoch_list,cost_list)

def predict(area, bedrooms, total, population,w,b):
  scaled_X = sx.transform([[area, bedrooms, total, population]])[0]
  scaled_price = w[0]*scaled_X[0] + w[1]*scaled_X[1] + b
  return sy.inverse_transform([[scaled_price]])[0][0]

column_averages = df.mean()
column_averages

predict(2600,4,1,column_averages[3],w,b)

predict(1000,2,1,499.539680,w,b)

predict(1500,3,1,1,w,b)

import random
random.randint(0,6)

def stochastic_gradient_descent(X, y_true, epochs, learning_rate =0.01):
  number_of_features = X.shape[1]
  w = np.ones(shape=(number_of_features))
  b = 0
  total_samples = X.shape[0]

  cost_list = []
  epoch_list = []

  for i in range(epochs):
    random_index = random.randint(0,total_samples-1)
    sample_x = X[random_index]
    sample_y = y_true[random_index]

    y_predicted = np.dot(w, sample_x.T) + b
    w_grad = -(2/total_samples)*(sample_x.T.dot(sample_y-y_predicted))
    b_grad = -(2/total_samples)*(sample_y-y_predicted)

    w = w - learning_rate * w_grad
    b = b - learning_rate * b_grad

    cost = np.square(sample_y-y_predicted) # MSE (Mean Squared Error)

    if i%10==0:
      cost_list.append(cost)
      epoch_list.append(i)
  return w, b, cost, cost_list, epoch_list

w_sgd, b_sgd, cost_sgd, cost_list_sgd, epoch_list_sgd = stochastic_gradient_descent(scaled_X,scaled_y.reshape(scaled_y.shape[0],),10000)
w_sgd, b_sgd, cost_sgd

w , b

plt.xlabel("epoch")
plt.ylabel("cost")
plt.plot(epoch_list_sgd,cost_list_sgd)

predict(2600,4,1,1,w_sgd, b_sgd)

predict(1000,2,1,1,w_sgd, b_sgd)

